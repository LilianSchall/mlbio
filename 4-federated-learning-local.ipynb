{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f40b296b-9a10-4c3d-ac98-0eb4c3e09303",
   "metadata": {},
   "source": [
    "# Federated Learning: Local context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "591c7544-42ad-46c3-8209-a04fb256d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c5846-d833-485e-970a-385b3c7ce5c1",
   "metadata": {},
   "source": [
    "## 1: MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1ff9ea-0e48-48f9-847c-b091f3dcd680",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "indices = torch.randperm(len(dataset))\n",
    "subset1_indices = indices[:600]\n",
    "subset2_indices = indices[600:1200]\n",
    "\n",
    "subset1 = DataLoader(dataset, sampler=SubsetRandomSampler(subset1_indices), batch_size=32)\n",
    "subset2 = DataLoader(dataset, sampler=SubsetRandomSampler(subset2_indices), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153540d5-a753-4db4-9488-30dc98064f4e",
   "metadata": {},
   "source": [
    "## 2: Definition of a simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ecaf82-9343-4bcc-be8c-94ad710e801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00df31-c8c5-429e-8472-f462bc0a92ee",
   "metadata": {},
   "source": [
    "## 3: Model parameter averaging function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad7f1732-25ba-4074-b49a-54a0c48fdbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_model_parameters(models, average_weight):\n",
    "    averaged_params = {}\n",
    "    for key in models[0].state_dict().keys():\n",
    "        averaged_params[key] = sum(weight * models[i].state_dict()[key] for i, weight in enumerate(average_weight))\n",
    "    return averaged_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a99a79-2762-42c4-ac67-bfb40a165c97",
   "metadata": {},
   "source": [
    "## 4: Update of Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88fd9ed5-6855-46bb-afa5-b9617f549535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(model, averaged_params):\n",
    "    model.load_state_dict(averaged_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3b3be9-ca09-439d-a0a2-21f312c93ba8",
   "metadata": {},
   "source": [
    "## 5: Federated Training Algorithm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62010fb8-3ec3-4765-95a1-378e96b9fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, optimizer, criterion, epochs=1):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def federated_training(model1, model2, subset1, subset2, epochs, common_params=True):\n",
    "    optimizer1 = torch.optim.Adam(model1.parameters())\n",
    "    optimizer2 = torch.optim.Adam(model2.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_model(model1, subset1, optimizer1, criterion)\n",
    "        train_model(model2, subset2, optimizer2, criterion)\n",
    "\n",
    "        if common_params:\n",
    "            avg_params = average_model_parameters([model1, model2], [0.5, 0.5])\n",
    "            update_model(model1, avg_params)\n",
    "            update_model(model2, avg_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23e28e9-badf-4ec5-adc8-b0f21becc183",
   "metadata": {},
   "source": [
    "## 6: Training without common parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d61c7f1d-3706-4f77-95b6-f16f8a65c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a74a31-72bc-4ad5-8052-2e3f12cee793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training without common parameters\n",
      "Model 1 accuracy:  0.8952166666666667\n",
      "Model 2 accuracy:  0.8978166666666667\n"
     ]
    }
   ],
   "source": [
    "model1 = CNN().to(device)\n",
    "model2 = CNN().to(device)\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "federated_training(model1, model2, subset1, subset2, 10, common_params=False)\n",
    "print(\"Training without common parameters\")\n",
    "print(\"Model 1 accuracy: \", evaluate_model(model1, dataloader))\n",
    "print(\"Model 2 accuracy: \", evaluate_model(model2, dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa360e7-64f8-4830-ab26-48dc3bf779cd",
   "metadata": {},
   "source": [
    "## 7: Training with common parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ffd0cd2-061c-4e87-8b0e-422073c8216a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with common parameters\n",
      "Model 1 accuracy:  0.9102833333333333\n",
      "Model 2 accuracy:  0.9102833333333333\n"
     ]
    }
   ],
   "source": [
    "model1 = CNN().to(device)\n",
    "model2 = CNN().to(device)\n",
    "\n",
    "federated_training(model1, model2, subset1, subset2, 10, common_params=True)\n",
    "print(\"Training with common parameters\")\n",
    "print(\"Model 1 accuracy: \", evaluate_model(model1, dataloader))\n",
    "print(\"Model 2 accuracy: \", evaluate_model(model2, dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8724b-34dc-4551-8662-46974ae98df0",
   "metadata": {},
   "source": [
    "## 8: Impact of batch size on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83cef40f-94e4-40c7-b7a9-829de8771f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size 64, accuracy: 0.8915166666666666\n",
      "Batch Size 32, accuracy: 0.91775\n",
      "Batch Size 16, accuracy: 0.9244333333333333\n",
      "Batch Size 8, accuracy: 0.9345833333333333\n"
     ]
    }
   ],
   "source": [
    "for batch_size in [64, 32, 16, 8]:\n",
    "    subset1 = DataLoader(dataset, sampler=SubsetRandomSampler(subset1_indices), batch_size=batch_size)\n",
    "    subset2 = DataLoader(dataset, sampler=SubsetRandomSampler(subset2_indices), batch_size=batch_size)\n",
    "    model1 = CNN().to(device)\n",
    "    model2 = CNN().to(device)\n",
    "\n",
    "    federated_training(model1, model2, subset1, subset2, 10, common_params=True)\n",
    "    \n",
    "    accuracy = evaluate_model(model1, DataLoader(dataset, batch_size=batch_size))\n",
    "    print(f\"Batch Size {batch_size}, accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c573af-6315-46f6-bc7e-d9bab02ebbaa",
   "metadata": {},
   "source": [
    "## 9: Repeat Experiments on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e20316c-bafe-490f-9f1f-84aa597cc4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Batch Size 64, accuracy: 0.90405\n",
      "Batch Size 32, accuracy: 0.9110833333333334\n",
      "Batch Size 16, accuracy: 0.9192666666666667\n",
      "Batch Size 8, accuracy: 0.9361666666666667\n"
     ]
    }
   ],
   "source": [
    "transform_cifar = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "cifar_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_cifar)\n",
    "\n",
    "cifar_indices = torch.randperm(len(cifar_dataset))\n",
    "cifar_subset1_indices = cifar_indices[:600]\n",
    "cifar_subset2_indices = cifar_indices[600:1200]\n",
    "\n",
    "batch_sizes = [64, 32, 16, 8]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    subset1 = DataLoader(dataset, sampler=SubsetRandomSampler(cifar_subset1_indices), batch_size=batch_size)\n",
    "    subset2 = DataLoader(dataset, sampler=SubsetRandomSampler(cifar_subset2_indices), batch_size=batch_size)\n",
    "    model1 = CNN().to(device)\n",
    "    model2 = CNN().to(device)\n",
    "\n",
    "    federated_training(model1, model2, subset1, subset2, 10, common_params=True)\n",
    "    \n",
    "    accuracy = evaluate_model(model1, DataLoader(dataset, batch_size=batch_size))\n",
    "    print(f\"Batch Size {batch_size}, accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b96093d-bcea-4994-9108-fccd32c07813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
